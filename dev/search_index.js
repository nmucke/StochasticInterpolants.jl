var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = StochasticInterpolants","category":"page"},{"location":"#StochasticInterpolants","page":"Home","title":"StochasticInterpolants","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for StochasticInterpolants.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [StochasticInterpolants]","category":"page"},{"location":"#StochasticInterpolants.StochasticInterpolants","page":"Home","title":"StochasticInterpolants.StochasticInterpolants","text":"StochasticInterpolants\n\nImplementation of the Stochastic Interpolants method for generative modeling.\n\n\n\n\n\n","category":"module"},{"location":"#StochasticInterpolants.AttnParsConvNextUNet","page":"Home","title":"StochasticInterpolants.AttnParsConvNextUNet","text":"AttnParsConvNextUNet(         imagesize::Tuple{Int, Int},         channels::Vector{Int} = [32, 64, 96, 128],         blockdepth::Int = 2,         minfreq::Float32 = 1.0f0,         maxfreq::Float32 = 1000.0f0,         embedding_dims::Int = 32     )\n\nCreate a conditional U-Net model with the given image size, number of channels, block depth, frequency range, and embedding dimensions.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.Autoencoder","page":"Home","title":"StochasticInterpolants.Autoencoder","text":"Autoencoder(in_channels::Int = 3, channels=[32, 64, 128], padding=\"periodic\")\n\nCreate an autoencoder model with the given number of input channels, output channels, and padding type.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.ConditionalDiffusionTransformer","page":"Home","title":"StochasticInterpolants.ConditionalDiffusionTransformer","text":"ConditionalDiffusionTransformer(     imsize::Dims{2}=(256, 256),     inchannels::Int=3,     patchsize::Dims{2}=(16, 16),     embedplanes::Int=768,     depth::Int=6,     numberheads=16,     mlpratio=4.0f0,     dropoutrate=0.1f0,     embeddingdropoutrate=0.1f0,     pool::Symbol=:class,     num_classes::Int=1000,     kwargs... )\n\nCreate a DiffusionTransformer Transformer model with the given image size, number of input channels, patch size, embedding planes, depth, number of heads, MLP ratio, dropout rate, embedding dropout rate, pooling method, number of classes, and additional keyword arguments.\n\nBased on https://github.com/LuxDL/Boltz.jl/blob/v0.3.9/src/vision/vit.jl#L48-L61 The architecture is based on https://arxiv.org/abs/2212.09748\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.ConditionalTransformerUNet","page":"Home","title":"StochasticInterpolants.ConditionalTransformerUNet","text":"ConditionalTransformerUNet(\n    image_size::Tuple{Int, Int},\n    channels::Vector{Int} = [32, 64, 96, 128],\n    block_depth::Int = 2,\n    min_freq::Float32 = 1.0f0,\n    max_freq::Float32 = 1000.0f0,\n    embedding_dims::Int = 32\n)\n\nCreate a conditional U-Net model with the given image size, number of channels, block depth, frequency range, and embedding dimensions.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.ConvNextUNetNoPars","page":"Home","title":"StochasticInterpolants.ConvNextUNetNoPars","text":"ConvNextUNetNoPars(         imagesize::Tuple{Int, Int},         channels::Vector{Int} = [32, 64, 96, 128],         blockdepth::Int = 2,         minfreq::Float32 = 1.0f0,         maxfreq::Float32 = 1000.0f0,     )\n\nCreate a conditional U-Net model with the given image size, number of channels, block depth, frequency range, and embedding dimensions.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.ConvNextUNetWithPars","page":"Home","title":"StochasticInterpolants.ConvNextUNetWithPars","text":"ConvNextUNetWithPars(         imagesize::Tuple{Int, Int},         channels::Vector{Int} = [32, 64, 96, 128],         blockdepth::Int = 2,         minfreq::Float32 = 1.0f0,         maxfreq::Float32 = 1000.0f0,         embedding_dims::Int = 32     )\n\nCreate a conditional U-Net model with the given image size, number of channels, block depth, frequency range, and embedding dimensions.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.DataDependentCouplingStochasticInterpolant","page":"Home","title":"StochasticInterpolants.DataDependentCouplingStochasticInterpolant","text":"DataDependentCouplingStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer\n    score::Lux.AbstractExplicitLayer\n\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.DataDependentCouplingStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer, LuxCore.AbstractExplicitLayer, LuxCore.AbstractExplicitLayer}","page":"Home","title":"StochasticInterpolants.DataDependentCouplingStochasticInterpolant","text":"DataDependentCouplingStochasticInterpolant(         velocity::Lux.AbstractExplicitLayer;          interpolant=Interpolant(),         gamma=Gamma(),         diffusioncoefficient=DiffusionCoefficient(),         diffusionmultiplier=0.1f0,         dev=gpu_device()      interpolant::Interpolant     )\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.DataDependentCouplingStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer, LuxCore.AbstractExplicitLayer}","page":"Home","title":"StochasticInterpolants.DataDependentCouplingStochasticInterpolant","text":"DataDependentCouplingStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer; \n    interpolant=Interpolant(),\n    diffusion_multiplier=0.1f0,\n    dev=gpu_device() \n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.DenoisingDiffusionProbabilisticModel","page":"Home","title":"StochasticInterpolants.DenoisingDiffusionProbabilisticModel","text":"DenoisingDiffusionProbabilisticModel(\n    image_size::Tuple{Int, Int}; \n    in_channels::Int = 3,\n    channels=[32, 64, 96, 128], \n    block_depth=2,\n    min_freq=1.0f0, \n    max_freq=1000.0f0, \n    embedding_dims=32,\n    timesteps::Int=100,\n    init_beta::AbstractFloat=0.0001, \n    final_beta::AbstractFloat=0.02,\n    type::String=\"linear\"\n)\n\nCreates a Denoising Diffusion Probabilistic Model (DDPM) with a UNet architecture.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.DiffusionTransformer","page":"Home","title":"StochasticInterpolants.DiffusionTransformer","text":"DiffusionTransformer(\n    imsize::Dims{2}=(256, 256),\n    in_channels::Int=3,\n    patch_size::Dims{2}=(16, 16),\n    embed_planes::Int=768,\n    depth::Int=6,\n    number_heads=16,\n    mlp_ratio=4.0f0,\n    dropout_rate=0.1f0,\n    embedding_dropout_rate=0.1f0,\n    pool::Symbol=:class,\n    num_classes::Int=1000,\n    kwargs...\n)\n\nCreate a DiffusionTransformer Transformer model with the given image size, number of input channels, patch size, embedding planes, depth, number of heads, MLP ratio, dropout rate, embedding dropout rate, pooling method, number of classes, and additional keyword arguments.\n\nBased on https://github.com/LuxDL/Boltz.jl/blob/v0.3.9/src/vision/vit.jl#L48-L61 The architecture is based on https://arxiv.org/abs/2212.09748\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.DiffusionTransformerBlock","page":"Home","title":"StochasticInterpolants.DiffusionTransformerBlock","text":"DiffusionTransformerBlock(\n    in_channels::Int, \n    out_channels::Int,\n    block_depth::Int\n)\n\nCreate a diffusion transformer block\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.EncoderFollmerStochasticInterpolant","page":"Home","title":"StochasticInterpolants.EncoderFollmerStochasticInterpolant","text":"EncoderFollmerStochasticInterpolant(\n    unet::UNet,\n    sde_sample::Function,\n    ode_sample::Function,\n    interpolant::Function\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.EncoderFollmerStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer, LuxCore.AbstractExplicitLayer, NamedTuple}","page":"Home","title":"StochasticInterpolants.EncoderFollmerStochasticInterpolant","text":"EncoderFollmerStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer; \n    interpolant=Interpolant(),\n    diffusion_multiplier=0.1f0,\n    dev=gpu_device() \n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.EntropicActionMatching","page":"Home","title":"StochasticInterpolants.EntropicActionMatching","text":"FollmerStochasticInterpolant(\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.EntropicActionMatching-Tuple{LuxCore.AbstractExplicitLayer}","page":"Home","title":"StochasticInterpolants.EntropicActionMatching","text":"EntropicActionMatching(\n    velocity::Lux.AbstractExplicitLayer; \n    diffusion_coefficient=DiffusionCoefficient(t -> sqrt.((3f0 .- t) .* (1f0 .- t))),\n    projection=nothing,\n    dev=gpu_device()\n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.FollmerStochasticInterpolant","page":"Home","title":"StochasticInterpolants.FollmerStochasticInterpolant","text":"FollmerStochasticInterpolant(\n    unet::UNet,\n    sde_sample::Function,\n    ode_sample::Function,\n    interpolant::Function\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.FollmerStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer}","page":"Home","title":"StochasticInterpolants.FollmerStochasticInterpolant","text":"FollmerStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer; \n    interpolant=Interpolant(),\n    diffusion_multiplier=0.1f0,\n    dev=gpu_device() \n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.LatentFollmerStochasticInterpolant","page":"Home","title":"StochasticInterpolants.LatentFollmerStochasticInterpolant","text":"LatentFollmerStochasticInterpolant(\n    unet::UNet,\n    sde_sample::Function,\n    ode_sample::Function,\n    interpolant::Function\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.LatentFollmerStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer, VAE_wrapper}","page":"Home","title":"StochasticInterpolants.LatentFollmerStochasticInterpolant","text":"LatentFollmerStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer,\n    autoencoder::Lux.AbstractExplicitLayer;\n    interpolant=Interpolant(),\n    diffusion_multiplier=0.1f0,\n    dev=gpu_device() \n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.NoiseScheduling","page":"Home","title":"StochasticInterpolants.NoiseScheduling","text":"NoiseScheduling\n\nA struct to hold the noise scheduling parameters.\n\nThis struct holds the parameters for the noise scheduling process.\n\nFields\n\nbetas::Vector{T}: The beta values.\nalphas::Vector{T}: The alpha values.\nalphas_cumprod::Vector{T}: The cumulative product of the alpha values.\nalphas_cumprod_prev::Vector{T}: The cumulative product of the alpha values, shifted by one.\nsqrt_recip_alphas::Vector{T}: The square root of the reciprocal of the alpha values.\nsqrt_alphas_cumprod::Vector{T}: The square root of the cumulative product of the alpha values.\nsqrt_one_minus_alphas_cumprod::Vector{T}: The square root of one minus the cumulative product of the alpha values.\nposterior_variance::Vector{T}: The posterior variance.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.NormalizePars","page":"Home","title":"StochasticInterpolants.NormalizePars","text":"NormalizePars(mean, std)\n\nA container for the normalization and inverse normalization functions. Data is transformed to be between 0 and 1.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.PhysicsInformedStochasticInterpolant","page":"Home","title":"StochasticInterpolants.PhysicsInformedStochasticInterpolant","text":"PhysicsInformedStochasticInterpolant(\n    unet::UNet,\n    sde_sample::Function,\n    ode_sample::Function,\n    interpolant::Function\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.PhysicsInformedStochasticInterpolant-Tuple{LuxCore.AbstractExplicitLayer, Function}","page":"Home","title":"StochasticInterpolants.PhysicsInformedStochasticInterpolant","text":"PhysicsInformedStochasticInterpolant(\n    velocity::Lux.AbstractExplicitLayer; \n    interpolant=Interpolant(),\n    diffusion_multiplier=0.1f0,\n    dev=gpu_device() \n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.PositionEncoding","page":"Home","title":"StochasticInterpolants.PositionEncoding","text":"position_encoding(dim_embedding::Int, max_length::Int=1000)\n\nCreate a position encoding for a transformer model.\n\nBased on https://liorsinai.github.io/machine-learning/2022/05/18/transformers.html#position-encodings\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.ScoreMatchingLangevinDynamics","page":"Home","title":"StochasticInterpolants.ScoreMatchingLangevinDynamics","text":"ScoreMatchingLangevinDynamics(         imagesize::Tuple{Int, Int};          inchannels::Int = 3,         channels=[32, 64, 96, 128],          blockdepth=2,         minfreq=1.0f0,          maxfreq=1000.0f0,          embeddingdims=32,         eps=1e-5,     )\n\nCreates a Denoising Diffusion Probabilistic Model (DDPM) with a UNet architecture.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.StandardizeData","page":"Home","title":"StochasticInterpolants.StandardizeData","text":"StandardizeData(mean, std)\n\nA container for the standardization and inverse standardization functions. Data is transform to have zero mean and unit variance.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.StochasticInterpolantModel","page":"Home","title":"StochasticInterpolants.StochasticInterpolantModel","text":"StochasticInterpolant(\n    unet::UNet,\n    sde_sample::Function,\n    ode_sample::Function,\n    interpolant::Function\n)\n\nA container layer for the Stochastic Interpolant model\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.StochasticInterpolantModel-Tuple{Tuple{Int64, Int64}}","page":"Home","title":"StochasticInterpolants.StochasticInterpolantModel","text":"StochasticInterpolantModel(\n    image_size::Tuple{Int, Int}; \n    in_channels::Int = 3,\n    channels=[32, 64, 96, 128], \n    block_depth=2,\n    min_freq=1.0f0,\n    max_freq=1000.0f0,\n    embedding_dims=32,\n    num_steps=100,\n)\n\nConstructs a Stochastic Interpolant model\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.ViPosEmbedding","page":"Home","title":"StochasticInterpolants.ViPosEmbedding","text":"ViPosEmbedding(embedding_size, number_patches; init = randn32)\n\nPositional embedding layer used by many vision transformer-like models.\n\n\n\n\n\n","category":"type"},{"location":"#StochasticInterpolants.CheckpointManager-Tuple{String, String}","page":"Home","title":"StochasticInterpolants.CheckpointManager","text":"CheckpointManager(\n    test_problem::String,\n    model_name::String,\n    config::Dict;\n    base_folder::String = \"trained_models\"\n)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.Interpolant","page":"Home","title":"StochasticInterpolants.Interpolant","text":"Interpolant(\n    alpha, \n    beta, \n    dalpha_dt, \n    dbeta_dt, \n    interpolant, \n    dinterpolant_dt\n)\n\nA struct that contains the functions for the interpolation and its derivatives.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.LinearMultiHeadSelfAttention-Union{Tuple{Int64, Int64}, Tuple{T}} where T","page":"Home","title":"StochasticInterpolants.LinearMultiHeadSelfAttention","text":"LinearMultiHeadSelfAttention(in_planes::Int, number_heads::Int; qkv_bias::Bool=false,\n    attention_dropout_rate::T=0.0f0, projection_dropout_rate::T=0.0f0)\n\nLinear Multi-head self-attention layer based on the paper \"Efficient Attention: Attention with Linear Complexities\"\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.MultiHeadSelfAttention-Union{Tuple{Int64, Int64}, Tuple{T}} where T","page":"Home","title":"StochasticInterpolants.MultiHeadSelfAttention","text":"MultiHeadSelfAttention(in_planes::Int, number_heads::Int; qkv_bias::Bool=false,\n    attention_dropout_rate::T=0.0f0, projection_dropout_rate::T=0.0f0)\n\nMulti-head self-attention layer\n\nArguments\n\nplanes: number of input channels\nnheads: number of heads\nqkv_bias: whether to use bias in the layer to get the query, key and value\nattn_dropout_prob: dropout probability after the self-attention layer\nproj_dropout_prob: dropout probability after the projection layer\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.VisionTransformerEncoder-Tuple{Any, Any, Any}","page":"Home","title":"StochasticInterpolants.VisionTransformerEncoder","text":"VisionTransformerEncoder(in_planes, depth, number_heads; mlp_ratio = 4.0f0,\n    dropout = 0.0f0)\n\nTransformer as used in the base ViT architecture.\n\nArguments\n\nin_planes: number of input channels\ndepth: number of attention blocks\nnumber_heads: number of attention heads\n\nKeyword Arguments\n\nmlp_ratio: ratio of MLP layers to the number of input channels\ndropout_rate: dropout rate\n\nReferences\n\n[1] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants._fast_chunk-Tuple{Int64, Int64}","page":"Home","title":"StochasticInterpolants._fast_chunk","text":"_fast_chunk(x::AbstractArray, ::Val{n}, ::Val{dim})\n\nType-stable and faster version of MLUtils.chunk.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants._flatten_spatial-Union{Tuple{AbstractArray{T, 4}}, Tuple{T}} where T","page":"Home","title":"StochasticInterpolants._flatten_spatial","text":"_flatten_spatial(x::AbstractArray{T, 4})\n\nFlattens the first 2 dimensions of x, and permutes the remaining dimensions to (2, 1, 3)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.conv_next_block-Tuple{}","page":"Home","title":"StochasticInterpolants.conv_next_block","text":"conv_next_block(\n    in_channels::Int, \n    out_channels::Int,\n    multiplier::Int = 1,\n    pars_embed_dim::Int = 1,\n    imsize::Tuple{Int, Int} = (64, 128)\n)\n\nCreate a conv next block with the given number of input and output channels.  The block consists of two convolutional layers with kernel size kernel_size. The first layer has the same number of input and output channels, while the  second layer has the same number of output channels as the block.  The block also includes batch normalization and a skip connection.\n\nhttps://arxiv.org/abs/2201.03545\n\nBased on https://github.com/tum-pbs/autoreg-pde-diffusion/blob/b9b33913b99ede88d9452c5ab470c5d7f5da5c56/src/turbpred/modeldiffusionblocks.py#L60\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.conv_next_block_no_pars-Tuple{}","page":"Home","title":"StochasticInterpolants.conv_next_block_no_pars","text":"conv_next_block_no_pars(\n    in_channels::Int, \n    out_channels::Int,\n    multiplier::Int = 1,\n    pars_embed_dim::Int = 1,\n    imsize::Tuple{Int, Int} = (64, 128)\n)\n\nCreate a conv next block with the given number of input and output channels.  The block consists of two convolutional layers with kernel size kernel_size. The first layer has the same number of input and output channels, while the  second layer has the same number of output channels as the block.  The block also includes batch normalization and a skip connection.\n\nhttps://arxiv.org/abs/2201.03545\n\nBased on https://github.com/tum-pbs/autoreg-pde-diffusion/blob/b9b33913b99ede88d9452c5ab470c5d7f5da5c56/src/turbpred/modeldiffusionblocks.py#L60\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.create_checkpoint_directory-Tuple{String, String}","page":"Home","title":"StochasticInterpolants.create_checkpoint_directory","text":"create_checkpoint_directory(\n    test_problem::String,\n    model_name::String;\n    base_folder::String = \"trained_models\"\n)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.euler_maruyama_sampler","page":"Home","title":"StochasticInterpolants.euler_maruyama_sampler","text":"Euler_Maruyama_sampler(\n    model::ScoreMatchingLangevinDynamics,\n    marginal_probability_std::Function,\n    ps::NamedTuple,\n    st::NamedTuple,\n    rng::AbstractRNG,\n    num_samples::Int,\n    num_steps::Int,\n    eps::Float32,\n    dev=gpu_device()\n)\n\nSamples from the SMLD model using the Euler-Maruyama method\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.forward_diffusion_sample","page":"Home","title":"StochasticInterpolants.forward_diffusion_sample","text":"forward_diffusion_sample(\n    x_0::AbstractArray, \n    t::AbstractArray, \n    rng::AbstractRNG, \n    noise_scheduling::NoiseScheduling, \n    dev=gpu_device()\n)\n\nSamples the forward diffusion process.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_beta_schedule-Tuple{Int64}","page":"Home","title":"StochasticInterpolants.get_beta_schedule","text":"getbetaschedule\n\nReturns a schedule of beta values for the diffusion process.\n\nBased on https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=qWw50ui9IZ5q\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.get_encoder_forecasting_loss","page":"Home","title":"StochasticInterpolants.get_encoder_forecasting_loss","text":"get_forecasting_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    pars::AbstractArray,\n    velocity::Lux.AbstractExplicitLayer,\n    encoder::Lux.AbstractExplicitLayer,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_forecasting_from_gaussian_loss","page":"Home","title":"StochasticInterpolants.get_forecasting_from_gaussian_loss","text":"get_forecasting_from_gaussian_loss(\n    x_1::AbstractArray,\n    x_condition::AbstractArray,\n    pars::AbstractArray,\n    velocity::Lux.AbstractExplicitLayer,\n    interpolant::NamedTuple, \n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_forecasting_loss","page":"Home","title":"StochasticInterpolants.get_forecasting_loss","text":"get_forecasting_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    pars::AbstractArray,\n    velocity::UNet,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_forecasting_loss-2","page":"Home","title":"StochasticInterpolants.get_forecasting_loss","text":"get_forecasting_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    pars::AbstractArray,\n    velocity::UNet,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_loss","page":"Home","title":"StochasticInterpolants.get_loss","text":"get_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    t::AbstractArray, \n    velocity::UNet,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_loss-2","page":"Home","title":"StochasticInterpolants.get_loss","text":"get_loss(\n    x_0::AbstractArray, \n    t::AbstractArray, \n    model::ScoreMatchingLangevinDynamics, \n    marginal_prob_std, \n    eps=1e-5,\n    dev=gpu_device()\n)\n\nComputes the loss of the SMLD model\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_loss-3","page":"Home","title":"StochasticInterpolants.get_loss","text":"get_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    t::AbstractArray, \n    velocity::UNet,\n    score::UNet,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_physics_forecasting_loss","page":"Home","title":"StochasticInterpolants.get_physics_forecasting_loss","text":"get_physics_forecasting_loss(\n    x_0::AbstractArray, \n    x_1::AbstractArray,\n    pars::AbstractArray,\n    velocity::UNet,\n    interpolant::Function, \n    gamma::Function,\n    ps::NamedTuple, \n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nComputes the loss for the stochastic interpolant Model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.get_t_pars_embedding","page":"Home","title":"StochasticInterpolants.get_t_pars_embedding","text":"get_t_pars_embedding(\n    pars_dim,\n    with_time,\n    embedding_dim,\n    min_freq=1.0f0, \n    max_freq=1000.0f0, \n)\n\nGet the time and parameter embedding layer.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.load_model_config-Tuple{String}","page":"Home","title":"StochasticInterpolants.load_model_config","text":"load_model_config(path::String)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.load_model_weights-Tuple{String}","page":"Home","title":"StochasticInterpolants.load_model_weights","text":"load_model_weights(path::String)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.multiple_conv_next_blocks-Tuple{}","page":"Home","title":"StochasticInterpolants.multiple_conv_next_blocks","text":"multiple_conv_next_blocks(\n    in_channels::Int, \n    out_channels::Int,\n    multiplier::Int = 1,\n    embedding_dims::Int = 1,\n    imsize::Tuple{Int, Int} = (64, 128)\n)\n\nCreate a chain of two conv next blocks with the given number of input and output channels. The first block has the same number of input and output\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.multiple_conv_next_blocks_no_pars-Tuple{}","page":"Home","title":"StochasticInterpolants.multiple_conv_next_blocks_no_pars","text":"multiple_conv_next_blocks_no_pars(\n    in_channels::Int, \n    out_channels::Int,\n    multiplier::Int = 1,\n    padding=\"constant\"\n)\n\nCreate a chain of two conv next blocks with the given number of input and output channels. The first block has the same number of input and output\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.parameter_diffusion_transformer_block-Tuple{}","page":"Home","title":"StochasticInterpolants.parameter_diffusion_transformer_block","text":"parameter_diffusion_transformer_block(\n    in_channels::Int,\n    out_channels::Int,\n    pars_dim::Int,\n    embed_dim::Int,\n    number_heads::Int,\n    mlp_ratio::Int,\n    imsize::Tuple{Int, Int},\n    patch_size::Int,\n    number_patches::Int\n)\n\nCreate a parameter diffusion transformer block with the given number of input and output channels, parameter dimensions, embedding dimensions, number of heads, MLP ratio, image size, patch size, and number of patches.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.patchify-Tuple{Tuple{Int64, Int64}}","page":"Home","title":"StochasticInterpolants.patchify","text":"patchify(\n    imsize::Tuple{Int, Int}=(64, 64),\n    in_channels::Int=3,\n    patch_size::Tuple{Int, Int}=(8, 8),\n    embed_planes::Int=128,\n    norm_layer=Returns(Lux.NoOpLayer()),\n    flatten=true\n)\n\nCreate a patch embedding layer with the given image size, number of input channels, patch size, embedding planes, normalization layer, and flatten flag.\n\nBased on https://github.com/LuxDL/Boltz.jl/blob/v0.3.9/src/vision/vit.jl#L48-L61\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.sample_timestep-Tuple{AbstractArray, AbstractArray}","page":"Home","title":"StochasticInterpolants.sample_timestep","text":"sample_timestep(\n    x::AbstractArray, \n    t::AbstractArray;\n    model,\n    noise_scheduling::NoiseScheduling,\n    ps::NamedTuple,\n    st::NamedTuple,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nCalls the model to predict the noise in the image and returns  the denoised image.  Applies noise to this image, if we are not in the last step yet.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.save_checkpoint-Tuple{}","page":"Home","title":"StochasticInterpolants.save_checkpoint","text":"save_checkpoint(ps, st, opt_st, output_dir, epoch)\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.sinusoidal_embedding","page":"Home","title":"StochasticInterpolants.sinusoidal_embedding","text":"sinusoidal_embedding(\n    x::AbstractArray{AbstractFloat, 4},\n    min_freq::AbstractFloat,\n    max_freq::AbstractFloat,\n    embedding_dims::Int\n)\n\nEmbed the noise variances to a sinusoidal embedding with the given frequency range and embedding dimensions.\n\nBased on https://yng87.page/en/blog/2022/lux-ddim/.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.train_diffusion_model","page":"Home","title":"StochasticInterpolants.train_diffusion_model","text":"train_diffusion_model(\n    model,\n    ps::NamedTuple,\n    st::NamedTuple,\n    opt_state::NamedTuple,\n    trainset::AbstractArray,\n    num_epochs::Int,\n    batch_size::Int,\n    num_samples::Int,\n    rng::AbstractRNG,\n    dev=gpu_device()\n)\n\nTrain the Diffusion model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.train_stochastic_interpolant","page":"Home","title":"StochasticInterpolants.train_stochastic_interpolant","text":"train_stochastic_interpolant(\n    model::StochasticInterpolantModel,\n    ps::NamedTuple,\n    st::NamedTuple,\n    opt_state::NamedTuple,\n    trainset::AbstractArray,\n    num_epochs::Int,\n    batch_size::Int,\n    num_samples::Int,\n    rng::AbstractRNG,\n    trainset_init_distribution::AbstractArray=nothing,\n    init_and_target_are_correlated=false,\n    train_time_stepping=false,\n    dev=gpu_device()\n)\n\nTrain the Stochastic Interpolant model.\n\n\n\n\n\n","category":"function"},{"location":"#StochasticInterpolants.train_stochastic_interpolant-Tuple{}","page":"Home","title":"StochasticInterpolants.train_stochastic_interpolant","text":"train_stochastic_interpolant(\n    model,\n    ps::NamedTuple,\n    st::NamedTuple,\n    opt_state::NamedTuple,\n    trainset::AbstractArray,\n    num_epochs::Int,\n    batch_size::Int,\n    num_samples::Int,\n    rng::AbstractRNG,\n    trainset_init_distribution::AbstractArray=nothing,\n    init_and_target_are_correlated=false,\n    train_time_stepping=false,\n    dev=gpu_device()\n)\n\nTrain the Stochastic Interpolant model.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.train_stochastic_interpolant_for_closure-Tuple{}","page":"Home","title":"StochasticInterpolants.train_stochastic_interpolant_for_closure","text":"train_stochastic_interpolant_for_closure(\n    model,\n    ps::NamedTuple,\n    st::NamedTuple,\n    opt_state::NamedTuple,\n    trainset::AbstractArray,\n    num_epochs::Int,\n    batch_size::Int,\n    num_samples::Int,\n    rng::AbstractRNG,\n    trainset_init_distribution::AbstractArray=nothing,\n    init_and_target_are_correlated=false,\n    train_time_stepping=false,\n    dev=gpu_device()\n)\n\nTrain the Stochastic Interpolant model.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticInterpolants.unpatchify-NTuple{4, Any}","page":"Home","title":"StochasticInterpolants.unpatchify","text":"unpatchify(x, patch_size, out_channels)\n\nUnpatchify the input tensor x with the given patch size and number of output channels.\n\n\n\n\n\n","category":"method"},{"location":"example/","page":"Example","title":"Example","text":"CurrentModule = StochasticInterpolants","category":"page"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"1+1","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"1+2","category":"page"}]
}


training_args:
  batch_size: 8
  num_epochs: 1000
  init_learning_rate: 1.0e-4
  min_learning_rate: 1.0e-6
  early_stop_patience: 10
  num_test_paths: 5
  num_test_sde_steps: 100
  test_frequency: 25

model_args:
  embedding_dims: 128
  channels: [16, 32, 64, 128]
  attention_type: "DiT" # "linear" or "standard" or "DiT"
  use_attention_in_layer: [false, false, false, false] # [true, true, true, true];
  attention_embedding_dims: 64
  num_heads: 4
  projection: null
  len_history: 2
  padding: "periodic"

optimizer_args:
  learning_rate: 1.0e-4
  weight_decay: 1.0e-8
  betas: [0.9, 0.999]

interpolant_args:
  alpha: "linear"
  beta: "quadratic"
  gamma: "linear"
  gamma_multiplier: 1.0e-1

diffusion_args:
  type: "follmer_optimal"
  multiplier: 1.0e-1
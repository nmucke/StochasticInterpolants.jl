
training_args:
  batch_size: 8
  num_epochs: 4000

model_args:
  embedding_dims: 128
  channels: [16, 32, 64, 128]
  attention_type: "DiT" # "linear" or "standard" or "DiT"
  use_attention_in_layer: [false, false, false, false] # [true, true, true, true];
  attention_embedding_dims: 64
  num_heads: 4
  projection: null

optimizer_args:
  learning_rate: 1.0e-4
  weight_decay: 1.0e-8
